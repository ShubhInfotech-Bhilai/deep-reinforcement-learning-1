{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0419 23:45:53.897444 140359473387328 registration.py:117] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 14.0   memory length: 15   epsilon: 0.9851045463620021\n",
      "episode: 1   score: 15.0   memory length: 31   epsilon: 0.9694605362958227\n",
      "episode: 2   score: 17.0   memory length: 49   epsilon: 0.9521577859830145\n",
      "episode: 3   score: 12.0   memory length: 62   epsilon: 0.9398537314349842\n",
      "episode: 4   score: 17.0   memory length: 80   epsilon: 0.9230793978373364\n",
      "episode: 5   score: 14.0   memory length: 95   epsilon: 0.9093297114626595\n",
      "episode: 6   score: 16.0   memory length: 112   epsilon: 0.8939941590229386\n",
      "episode: 7   score: 17.0   memory length: 130   epsilon: 0.8780383184956015\n",
      "episode: 8   score: 18.0   memory length: 149   epsilon: 0.8615048875706075\n",
      "episode: 9   score: 15.0   memory length: 165   epsilon: 0.847823709077432\n",
      "episode: 10   score: 17.0   memory length: 183   epsilon: 0.8326919101044725\n",
      "episode: 11   score: 12.0   memory length: 196   epsilon: 0.8219316276865206\n",
      "episode: 12   score: 8.0   memory length: 205   epsilon: 0.8145637636371417\n",
      "episode: 13   score: 12.0   memory length: 218   epsilon: 0.8040377382995522\n",
      "episode: 14   score: 22.0   memory length: 241   epsilon: 0.7857468750083979\n",
      "episode: 15   score: 10.0   memory length: 252   epsilon: 0.7771467460721305\n",
      "episode: 16   score: 14.0   memory length: 267   epsilon: 0.7655707927460921\n",
      "episode: 17   score: 21.0   memory length: 289   epsilon: 0.7489039087598284\n",
      "episode: 18   score: 15.0   memory length: 305   epsilon: 0.737010896662273\n",
      "episode: 19   score: 13.0   memory length: 319   epsilon: 0.7267595445649057\n",
      "episode: 20   score: 19.0   memory length: 339   epsilon: 0.7123616129911045\n",
      "episode: 21   score: 26.0   memory length: 366   epsilon: 0.6933758171534341\n",
      "episode: 22   score: 32.0   memory length: 399   epsilon: 0.6708567627695098\n",
      "episode: 23   score: 12.0   memory length: 412   epsilon: 0.6621877602947683\n",
      "episode: 24   score: 16.0   memory length: 429   epsilon: 0.6510201771893979\n",
      "episode: 25   score: 17.0   memory length: 447   epsilon: 0.6394008908411897\n",
      "episode: 26   score: 18.0   memory length: 466   epsilon: 0.6273609943589779\n",
      "episode: 27   score: 16.0   memory length: 483   epsilon: 0.6167807534338766\n",
      "episode: 28   score: 13.0   memory length: 497   epsilon: 0.6082017260423355\n",
      "episode: 29   score: 34.0   memory length: 532   epsilon: 0.5872725966265356\n",
      "episode: 30   score: 31.0   memory length: 564   epsilon: 0.5687682688709239\n",
      "episode: 31   score: 15.0   memory length: 580   epsilon: 0.5597359112837013\n",
      "episode: 32   score: 9.0   memory length: 590   epsilon: 0.5541636732359665\n",
      "episode: 33   score: 8.0   memory length: 599   epsilon: 0.5491961035890855\n",
      "episode: 34   score: 8.0   memory length: 608   epsilon: 0.54427306365317\n",
      "episode: 35   score: 15.0   memory length: 624   epsilon: 0.5356297035976458\n",
      "episode: 36   score: 10.0   memory length: 635   epsilon: 0.5297671482893791\n",
      "episode: 37   score: 17.0   memory length: 653   epsilon: 0.5203119633204238\n",
      "episode: 38   score: 10.0   memory length: 664   epsilon: 0.5146170632018707\n",
      "episode: 39   score: 12.0   memory length: 677   epsilon: 0.5079670346979859\n",
      "episode: 40   score: 11.0   memory length: 689   epsilon: 0.5019048446041944\n",
      "episode: 41   score: 12.0   memory length: 702   epsilon: 0.49541908701565013\n",
      "episode: 42   score: 9.0   memory length: 712   epsilon: 0.4904871306580321\n",
      "episode: 43   score: 12.0   memory length: 725   epsilon: 0.4841489160264175\n",
      "episode: 44   score: 10.0   memory length: 736   epsilon: 0.47884982641548285\n",
      "episode: 45   score: 17.0   memory length: 754   epsilon: 0.4703034042831738\n",
      "episode: 46   score: 11.0   memory length: 766   epsilon: 0.46469070022213765\n",
      "episode: 47   score: 9.0   memory length: 776   epsilon: 0.4600646486360102\n",
      "episode: 48   score: 10.0   memory length: 787   epsilon: 0.45502916529763104\n",
      "episode: 49   score: 14.0   memory length: 802   epsilon: 0.4482512994620033\n",
      "episode: 50   score: 13.0   memory length: 816   epsilon: 0.44201640942211684\n",
      "episode: 51   score: 14.0   memory length: 831   epsilon: 0.4354323744883354\n",
      "episode: 52   score: 13.0   memory length: 845   epsilon: 0.42937578752919014\n",
      "episode: 53   score: 18.0   memory length: 864   epsilon: 0.4212906564199442\n",
      "episode: 54   score: 11.0   memory length: 876   epsilon: 0.41626288125048977\n",
      "episode: 55   score: 11.0   memory length: 888   epsilon: 0.41129510865353336\n",
      "episode: 56   score: 11.0   memory length: 900   epsilon: 0.4063866225452039\n",
      "episode: 57   score: 10.0   memory length: 911   epsilon: 0.401938654041574\n",
      "episode: 58   score: 11.0   memory length: 923   epsilon: 0.3971418299163796\n",
      "episode: 59   score: 8.0   memory length: 932   epsilon: 0.3935818172430853\n",
      "episode: 60   score: 11.0   memory length: 944   epsilon: 0.3888847254426181\n",
      "episode: 61   score: 17.0   memory length: 962   epsilon: 0.38194398360437026\n",
      "episode: 62   score: 7.0   memory length: 970   epsilon: 0.3788991048049279\n",
      "episode: 63   score: 15.0   memory length: 986   epsilon: 0.37288197552507135\n",
      "episode: 64   score: 11.0   memory length: 998   epsilon: 0.36843192017940213\n",
      "episode: 65   score: 18.0   memory length: 1017   epsilon: 0.36149435996758156\n",
      "episode: 66   score: 9.0   memory length: 1027   epsilon: 0.35789564031060395\n",
      "episode: 67   score: 14.0   memory length: 1042   epsilon: 0.3525646223931159\n",
      "episode: 68   score: 14.0   memory length: 1057   epsilon: 0.34731301240586104\n",
      "episode: 69   score: 9.0   memory length: 1067   epsilon: 0.34385546976264747\n",
      "episode: 70   score: 46.0   memory length: 1114   epsilon: 0.3280604556381725\n",
      "episode: 71   score: 31.0   memory length: 1146   epsilon: 0.3177236235951741\n",
      "episode: 72   score: 28.0   memory length: 1175   epsilon: 0.30863748084864606\n",
      "episode: 73   score: 63.0   memory length: 1239   epsilon: 0.2894942299343089\n",
      "episode: 74   score: 53.0   memory length: 1293   epsilon: 0.2742687177907501\n",
      "episode: 75   score: 59.0   memory length: 1353   epsilon: 0.2582887971370021\n",
      "episode: 76   score: 49.0   memory length: 1403   epsilon: 0.24568575753695765\n",
      "episode: 77   score: 58.0   memory length: 1462   epsilon: 0.23160278977409607\n",
      "episode: 78   score: 65.0   memory length: 1528   epsilon: 0.21680336036122028\n",
      "episode: 79   score: 82.0   memory length: 1611   epsilon: 0.19952693536537153\n",
      "episode: 80   score: 43.0   memory length: 1655   epsilon: 0.1909338870261128\n",
      "episode: 81   score: 76.0   memory length: 1732   epsilon: 0.17677693812313813\n",
      "episode: 82   score: 54.0   memory length: 1787   epsilon: 0.16731214221728977\n",
      "episode: 83   score: 55.0   memory length: 1843   epsilon: 0.1581957458779228\n",
      "episode: 84   score: 48.0   memory length: 1892   epsilon: 0.15062731114705447\n",
      "episode: 85   score: 84.0   memory length: 1977   epsilon: 0.13834715185308472\n",
      "episode: 86   score: 80.0   memory length: 2000   epsilon: 0.1275777002194309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 87   score: 113.0   memory length: 2000   epsilon: 0.11382572543383399\n",
      "episode: 88   score: 54.0   memory length: 2000   epsilon: 0.10773139394747269\n",
      "episode: 89   score: 97.0   memory length: 2000   epsilon: 0.09766976109371296\n",
      "episode: 90   score: 80.0   memory length: 2000   epsilon: 0.09006678731304377\n",
      "episode: 91   score: 100.0   memory length: 2000   epsilon: 0.08141023015472601\n",
      "episode: 92   score: 49.0   memory length: 2000   epsilon: 0.07743786911599104\n",
      "episode: 93   score: 48.0   memory length: 2000   epsilon: 0.07373307000872446\n",
      "episode: 94   score: 50.0   memory length: 2000   epsilon: 0.07006517586536791\n",
      "episode: 95   score: 68.0   memory length: 2000   epsilon: 0.06539144043072388\n",
      "episode: 96   score: 79.0   memory length: 2000   epsilon: 0.06036149145650866\n",
      "episode: 97   score: 86.0   memory length: 2000   epsilon: 0.05532958818117005\n",
      "episode: 98   score: 106.0   memory length: 2000   epsilon: 0.04971239399803625\n",
      "episode: 99   score: 36.0   memory length: 2000   epsilon: 0.047905760870910884\n",
      "episode: 100   score: 64.0   memory length: 2000   epsilon: 0.04488946991562231\n",
      "episode: 101   score: 75.0   memory length: 2000   epsilon: 0.041602706234017746\n",
      "episode: 102   score: 89.0   memory length: 2000   epsilon: 0.038020298525500304\n",
      "episode: 103   score: 57.0   memory length: 2000   epsilon: 0.03587681156833877\n",
      "episode: 104   score: 50.0   memory length: 2000   epsilon: 0.03409209886048263\n",
      "episode: 105   score: 117.0   memory length: 2000   epsilon: 0.030295724989556416\n",
      "episode: 106   score: 210.0   memory length: 2000   epsilon: 0.024530102644801786\n",
      "episode: 107   score: 80.0   memory length: 2000   epsilon: 0.02262058914587343\n",
      "episode: 108   score: 117.0   memory length: 2000   epsilon: 0.020101641458616367\n",
      "episode: 109   score: 83.0   memory length: 2000   epsilon: 0.018481300707052454\n",
      "episode: 110   score: 342.0   memory length: 2000   epsilon: 0.013112786644688904\n",
      "episode: 111   score: 170.0   memory length: 2000   epsilon: 0.01105079395877572\n",
      "episode: 112   score: 134.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 113   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 114   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 115   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 116   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 117   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 118   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 119   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 120   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 121   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 122   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourcecode369/anaconda3/envs/drlnd/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3299: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "EPISODES = 300\n",
    "\n",
    "\n",
    "# DQN Agent for the Cartpole\n",
    "# it uses Neural Network to approximate q function\n",
    "# and replay memory & target q network\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/cartpole_dqn.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    np.amax(target_val[i]))\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, maximum length of episode is 500\n",
    "    env = gym.make('CartPole-v1')\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            reward = reward if not done or score == 499 else -100\n",
    "\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            agent.train_model()\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.update_target_model()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "#                 pylab.plot(episodes, scores, 'b')\n",
    "#                 pylab.savefig(\"./save_graph/cartpole_dqn.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    sys.exit()\n",
    "\n",
    "        # save the model\n",
    "#         if e % 50 == 0:\n",
    "#             agent.model.save_weights(\"./save_model/cartpole_dqn.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
